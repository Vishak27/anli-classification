{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLI - Data Preprocessing\n",
    "\n",
    "This notebook handles data preprocessing for the ANLI dataset.\n",
    "Note: ANLI is already quite well-preprocessed, so minimal cleaning was needed.\n",
    "\n",
    "Focus:\n",
    "- Verify data quality\n",
    "- Check for missing values\n",
    "- Prepare data loaders for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_loader import ANLIDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ANLI dataset:\n",
      "Train size: 45460\n",
      "Dev size: 1000\n",
      "Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "loader = ANLIDataLoader()\n",
    "train_df, dev_df, test_df = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values check:\n",
      "\n",
      "Train set:\n",
      "uid           0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "reason        0\n",
      "dtype: int64\n",
      "\n",
      "Dev set:\n",
      "uid           0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "reason        0\n",
      "dtype: int64\n",
      "\n",
      "Test set:\n",
      "uid           0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "reason        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values check:\")\n",
    "print(\"\\nTrain set:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nDev set:\")\n",
    "print(dev_df.isnull().sum())\n",
    "print(\"\\nTest set:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate check:\n",
      "Train duplicates: 31\n",
      "Dev duplicates: 0\n",
      "Test duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate entries\n",
    "print(\"\\nDuplicate check:\")\n",
    "print(f\"Train duplicates: {train_df.duplicated(subset=['premise', 'hypothesis']).sum()}\")\n",
    "print(f\"Dev duplicates: {dev_df.duplicated(subset=['premise', 'hypothesis']).sum()}\")\n",
    "print(f\"Test duplicates: {test_df.duplicated(subset=['premise', 'hypothesis']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution:\n",
      "\n",
      "Train:\n",
      "label\n",
      "0    14448\n",
      "1    20959\n",
      "2    10053\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dev:\n",
      "label\n",
      "0    334\n",
      "1    333\n",
      "2    333\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test:\n",
      "label\n",
      "0    334\n",
      "1    333\n",
      "2    333\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(\"\\nTrain:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "print(\"\\nDev:\")\n",
    "print(dev_df['label'].value_counts().sort_index())\n",
    "print(\"\\nTest:\")\n",
    "print(test_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared 45460 training pairs\n",
      "Prepared 1000 dev pairs\n",
      "Prepared 1000 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Prepare text pairs\n",
    "train_premises, train_hypotheses = loader.prepare_text_pairs(train_df)\n",
    "dev_premises, dev_hypotheses = loader.prepare_text_pairs(dev_df)\n",
    "test_premises, test_hypotheses = loader.prepare_text_pairs(test_df)\n",
    "\n",
    "train_labels = train_df['label'].values\n",
    "dev_labels = dev_df['label'].values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "print(f\"\\nPrepared {len(train_labels)} training pairs\")\n",
    "print(f\"Prepared {len(dev_labels)} dev pairs\")\n",
    "print(f\"Prepared {len(test_labels)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data integrity check:\n",
      "All labels in valid range: True\n",
      "No empty premises: True\n",
      "No empty hypotheses: True\n"
     ]
    }
   ],
   "source": [
    "# Verify data integrity\n",
    "print(\"\\nData integrity check:\")\n",
    "print(f\"All labels in valid range: {all((train_df['label'] >= 0) & (train_df['label'] <= 2))}\")\n",
    "print(f\"No empty premises: {all(train_df['premise'].str.len() > 0)}\")\n",
    "print(f\"No empty hypotheses: {all(train_df['hypothesis'].str.len() > 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessed data saved to ../data/processed/\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data (optional)\n",
    "train_df.to_csv('../data/processed/train_r2.csv', index=False)\n",
    "dev_df.to_csv('../data/processed/dev_r2.csv', index=False)\n",
    "test_df.to_csv('../data/processed/test_r2.csv', index=False)\n",
    "\n",
    "print(\"\\n Preprocessed data saved to ../data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anli-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
